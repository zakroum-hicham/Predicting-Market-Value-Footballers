{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score =  0.8911476332957085\n",
      "mean_absolute_error  =  848448.975500891\n",
      "root_mean_squared_error =  1347716.9512283776\n",
      "mean_absolute_percentage_error % 100 =  72.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error ,mean_absolute_percentage_error,root_mean_squared_error\n",
    "\n",
    "target =df.new_values\n",
    "dff =  df.drop(\"new_values\",axis=1)\n",
    "clf = LinearRegression()\n",
    "clf.fit(dff,target)\n",
    "score= clf.score(dff,target)\n",
    "prediction = clf.predict(dff)\n",
    "\n",
    "err = mean_absolute_error(target,prediction)\n",
    "\n",
    "print(\"score = \",score)\n",
    "print(\"mean_absolute_error  = \",err)\n",
    "print(\"root_mean_squared_error = \",root_mean_squared_error(target,prediction))\n",
    "print(\"mean_absolute_percentage_error % 100 = \",mean_absolute_percentage_error(target,prediction)%100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R² score: 0.9763930673012313\n",
      "Random Forest MAE: 301890.4914529915\n",
      "Random Forest RMSE: 641449.1613354674\n",
      "Random Forest MAPE: 7.60706889398606%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = df[df[\"new_values\"]> 10000]\n",
    "X_train,X_test,y_train,y_test  = train_test_split(df.drop(\"new_values\",axis=1),df.new_values,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=90, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "rf_mape = mean_absolute_percentage_error(y_test, y_pred_rf) * 100\n",
    "rf_r2 = rf_model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Random Forest R² score: {rf_r2}\")\n",
    "print(f\"Random Forest MAE: {rf_mae}\")\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest MAPE: {rf_mape}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting R² score: 0.97922838127013\n",
      "Gradient Boosting MAE: 338843.81543636246\n",
      "Gradient Boosting RMSE: 601696.7031530134\n",
      "Gradient Boosting MAPE: 13.266275252723162%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=90, random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "gb_mae = mean_absolute_error(y_test, y_pred_gb)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "gb_mape = mean_absolute_percentage_error(y_test, y_pred_gb) * 100\n",
    "gb_r2 = gb_model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Gradient Boosting R² score: {gb_r2}\")\n",
    "print(f\"Gradient Boosting MAE: {gb_mae}\")\n",
    "print(f\"Gradient Boosting RMSE: {gb_rmse}\")\n",
    "print(f\"Gradient Boosting MAPE: {gb_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression R² score: 0.9183168765542836\n",
      "Polynomial Regression MAE: 757564.4713349532\n",
      "Polynomial Regression RMSE: 1193187.3181687598\n",
      "Polynomial Regression MAPE: 63.13026471747391%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transform features into polynomial features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Train a Linear Regression model on the polynomial features\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "poly_mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "poly_rmse = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
    "poly_mape = mean_absolute_percentage_error(y_test, y_pred_poly) * 100\n",
    "poly_r2 = poly_model.score(X_test_poly, y_test)\n",
    "\n",
    "print(f\"Polynomial Regression R² score: {poly_r2}\")\n",
    "print(f\"Polynomial Regression MAE: {poly_mae}\")\n",
    "print(f\"Polynomial Regression RMSE: {poly_rmse}\")\n",
    "print(f\"Polynomial Regression MAPE: {poly_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression R² score: 0.8677136806156032\n",
      "Lasso Regression MAE: 915938.1868647187\n",
      "Lasso Regression RMSE: 1518448.0119752502\n",
      "Lasso Regression MAPE: 50.49507460269541%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.437e+15, tolerance: 4.814e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize and train the Lasso Regression model\n",
    "lasso_model = Lasso(alpha=0.6)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "lasso_mae = mean_absolute_error(y_test, y_pred_lasso)\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "lasso_mape = mean_absolute_percentage_error(y_test, y_pred_lasso) * 100\n",
    "lasso_r2 = lasso_model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Lasso Regression R² score: {lasso_r2}\")\n",
    "print(f\"Lasso Regression MAE: {lasso_mae}\")\n",
    "print(f\"Lasso Regression RMSE: {lasso_rmse}\")\n",
    "print(f\"Lasso Regression MAPE: {lasso_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R² score: 0.8676994942538623\n",
      "Ridge Regression MAE: 916256.652061203\n",
      "Ridge Regression RMSE: 1518529.4288420654\n",
      "Ridge Regression MAPE: 50.54519746171425%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize and train the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=0.1) \n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "ridge_mape = mean_absolute_percentage_error(y_test, y_pred_ridge) * 100\n",
    "ridge_r2 = ridge_model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Ridge Regression R² score: {ridge_r2}\")\n",
    "print(f\"Ridge Regression MAE: {ridge_mae}\")\n",
    "print(f\"Ridge Regression RMSE: {ridge_rmse}\")\n",
    "print(f\"Ridge Regression MAPE: {ridge_mape}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "75 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "55 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.93132625 0.94407542 0.94034456 0.92678236        nan 0.92910677\n",
      " 0.95794619 0.92892104 0.95595103 0.94228863 0.94244674        nan\n",
      " 0.94264955        nan        nan 0.93669441 0.94250437 0.93900296\n",
      " 0.95365344 0.92253085        nan        nan 0.95190133 0.95894567\n",
      " 0.92944652        nan        nan 0.93163991        nan 0.95028472\n",
      "        nan        nan 0.95399896 0.95094761 0.93832114        nan\n",
      " 0.95810508        nan        nan 0.93685356 0.93669441 0.92389232\n",
      " 0.94723098 0.93148296 0.93453384 0.92467033 0.94862112 0.94944738\n",
      " 0.95384153        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n",
      "Tuned Random Forest R²: 0.9614643429791322\n",
      "Tuned Random Forest MAE: 419350.61180709105\n",
      "Tuned Random Forest RMSE: 819546.9619183242\n",
      "Tuned Random Forest MAPE: 15.161029113119866%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 800],      # Number of trees\n",
    "    'max_depth': [10, 20, 30, None],           # Depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],           # Minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2, 4],             # Minimum samples per leaf\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at every split\n",
    "    'bootstrap': [True, False]                 # Whether to use bootstrap samples\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rf_random_search = RandomizedSearchCV(estimator=rf_model, \n",
    "                                      param_distributions=rf_param_grid, \n",
    "                                      n_iter=50,  # Number of parameter settings to sample\n",
    "                                      cv=5,       # 5-fold cross-validation\n",
    "                                      verbose=2, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=42)\n",
    "\n",
    "# Fit the model to find the best parameters\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_rf_params = rf_random_search.best_params_\n",
    "print(f\"Best Random Forest Parameters: {best_rf_params}\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "best_rf_model = rf_random_search.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_r2 = best_rf_model.score(X_test, y_test)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "rf_mape = mean_absolute_percentage_error(y_test, y_pred_rf) * 100\n",
    "\n",
    "print(f\"Tuned Random Forest R²: {rf_r2}\")\n",
    "print(f\"Tuned Random Forest MAE: {rf_mae}\")\n",
    "print(f\"Tuned Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Tuned Random Forest MAPE: {rf_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Gradient Boosting Parameters: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Tuned Gradient Boosting R²: 0.983325723577597\n",
      "Tuned Gradient Boosting MAE: 259915.50407469698\n",
      "Tuned Gradient Boosting RMSE: 539095.8134079496\n",
      "Tuned Gradient Boosting MAPE: 7.3930605105371265%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define parameter grid\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 300, 500],        # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 0.05],     # Learning rate\n",
    "    'max_depth': [3, 5, 7],                 # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],        # Minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2, 4]           # Minimum samples per leaf\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "gb_random_search = RandomizedSearchCV(estimator=gb_model, \n",
    "                                      param_distributions=gb_param_grid, \n",
    "                                      n_iter=50, \n",
    "                                      cv=5, \n",
    "                                      verbose=2, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_gb_params = gb_random_search.best_params_\n",
    "print(f\"Best Gradient Boosting Parameters: {best_gb_params}\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "best_gb_model = gb_random_search.best_estimator_\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "gb_r2 = best_gb_model.score(X_test, y_test)\n",
    "gb_mae = mean_absolute_error(y_test, y_pred_gb)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "gb_mape = mean_absolute_percentage_error(y_test, y_pred_gb) * 100\n",
    "\n",
    "print(f\"Tuned Gradient Boosting R²: {gb_r2}\")\n",
    "print(f\"Tuned Gradient Boosting MAE: {gb_mae}\")\n",
    "print(f\"Tuned Gradient Boosting RMSE: {gb_rmse}\")\n",
    "print(f\"Tuned Gradient Boosting MAPE: {gb_mape}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rf_importances):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.001\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m         rf_importances_1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrf_feature_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf_importances_1)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\computer_vision\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5389\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5387\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5388\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5392\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5393\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8 is out of bounds for axis 0 with size 8"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Random Forest\n",
    "rf_importances = best_rf_model.feature_importances_\n",
    "rf_feature_names = X_train.columns\n",
    "rf_importances_1 = []\n",
    "for i,v in enumerate(rf_importances):\n",
    "    if v>0.001:\n",
    "        rf_importances_1.append(rf_feature_names[i])\n",
    "print(rf_importances_1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(rf_feature_names, rf_importances)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# For Gradient Boosting\n",
    "gb_importances = best_gb_model.feature_importances_\n",
    "gb_importances_1 = []\n",
    "for i,v in enumerate(gb_importances):\n",
    "    if v>0.001:\n",
    "        gb_importances_1.append(rf_feature_names[i])\n",
    "# print(gb_importances_1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(rf_feature_names, gb_importances)\n",
    "plt.title('Gradient Boosting Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradient Boosting with Most Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```  \n",
    "['Age',\n",
    " 'Overall rating',\n",
    " 'Potential',\n",
    " 'Best overall',\n",
    " 'Growth',\n",
    " 'Dribbling / Reflexes',\n",
    " 'new_wages',\n",
    " 'new_release_clause'] \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting with most important features R²: 0.9876771843995457\n",
      "Gradient Boosting with most important features MAE: 231958.47606122182\n",
      "Gradient Boosting with most important features RMSE: 463444.18316429236\n",
      "Gradient Boosting  with most important features MAPE: 6.8096062452372905%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = df[gb_importances_1+[\"new_values\"]]\n",
    "X_train,X_test,y_train,y_test  = train_test_split(df_test.drop(\"new_values\",axis=1),df_test.new_values,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=500, min_samples_split=5, min_samples_leaf=1, \n",
    "                                      max_depth=5, learning_rate=0.1, random_state=42)\n",
    "# Fit the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "gb_r2 = gb_model.score(X_test, y_test)\n",
    "gb_mae = mean_absolute_error(y_test, y_pred_gb)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "gb_mape = mean_absolute_percentage_error(y_test, y_pred_gb) * 100\n",
    "\n",
    "print(f\"Gradient Boosting with most important features R²: {gb_r2}\")\n",
    "print(f\"Gradient Boosting with most important features MAE: {gb_mae}\")\n",
    "print(f\"Gradient Boosting with most important features RMSE: {gb_rmse}\")\n",
    "print(f\"Gradient Boosting  with most important features MAPE: {gb_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_boosting_with_most_important_features_best_model.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Assuming your model is called `best_model`\n",
    "dump(gb_model, 'gradient_boosting_with_most_important_features_best_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor R²: 0.9779440056036691\n",
      "Voting Regressor MAE: 305650.5666188006\n",
      "Voting Regressor RMSE: 620020.1189386031\n",
      "Voting Regressor MAPE: 10.004210203158102%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestRegressor(n_estimators=300, min_samples_split=5, min_samples_leaf=1, \n",
    "                                  max_features='sqrt', max_depth=20, bootstrap=False, random_state=42)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=500, min_samples_split=5, min_samples_leaf=1, \n",
    "                                      max_depth=5, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Create a Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', rf_model), ('gb', gb_model)])\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_voting = voting_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "voting_mae = mean_absolute_error(y_test, y_pred_voting)\n",
    "voting_rmse = np.sqrt(mean_squared_error(y_test, y_pred_voting))\n",
    "voting_mape = mean_absolute_percentage_error(y_test, y_pred_voting) * 100\n",
    "voting_r2 = voting_regressor.score(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Voting Regressor R²: {voting_r2}\")\n",
    "print(f\"Voting Regressor MAE: {voting_mae}\")\n",
    "print(f\"Voting Regressor RMSE: {voting_rmse}\")\n",
    "print(f\"Voting Regressor MAPE: {voting_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor R²: 0.9880736515105122\n",
      "Stacking Regressor MAE: 226719.20754675046\n",
      "Stacking Regressor RMSE: 455927.94114205084\n",
      "Stacking Regressor MAPE: 6.7233399682774575%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the base models\n",
    "base_models = [\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model)\n",
    "]\n",
    "\n",
    "# Create the stacking regressor with a linear regression model as the final estimator\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_stacking = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "stacking_mae = mean_absolute_error(y_test, y_pred_stacking)\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_test, y_pred_stacking))\n",
    "stacking_mape = mean_absolute_percentage_error(y_test, y_pred_stacking) * 100\n",
    "stacking_r2 = stacking_regressor.score(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Stacking Regressor R²: {stacking_r2}\")\n",
    "print(f\"Stacking Regressor MAE: {stacking_mae}\")\n",
    "print(f\"Stacking Regressor RMSE: {stacking_rmse}\")\n",
    "print(f\"Stacking Regressor MAPE: {stacking_mape}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
